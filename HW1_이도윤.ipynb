{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Archi2903/AI-in-Engenery/blob/main/HW1_%EC%9D%B4%EB%8F%84%EC%9C%A4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e418cc90",
      "metadata": {
        "id": "e418cc90"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a3ee4cc",
      "metadata": {
        "id": "4a3ee4cc"
      },
      "source": [
        "Energy resources and artificial intelligence 이도윤22251844\n",
        "\n",
        "HW#01 - XOR model\n",
        "\n",
        "내용:\n",
        "Using Keras (tf.keras), train and compare a single-layer perceptron and a small MLP (e.g. one hidden layer) to learn the logical gates AND, OR, and XOR on the four 2-bit inputs.\n",
        "\n",
        "Report final accuracy for each gate, show the models’ predictions (and optionally a loss curve).\n",
        "\n",
        "Submit the jupyter notebook file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b306f94",
      "metadata": {
        "id": "3b306f94"
      },
      "outputs": [],
      "source": [
        "# Set random seeds for reproducibility 42\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37d41958",
      "metadata": {
        "id": "37d41958"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e810ce4",
      "metadata": {
        "id": "3e810ce4"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cb2cae0",
      "metadata": {
        "id": "4cb2cae0",
        "outputId": "11eb78e2-9e9c-4c4a-8cbc-30573b28be1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logical gate \n",
            "AND - probabilities and rounding (threshold 0.5)\n",
            "0 AND 0 -> prob=0.4701 pred=0\n",
            "0 AND 1 -> prob=0.7014 pred=1\n",
            "1 AND 0 -> prob=0.2523 pred=0\n",
            "1 AND 1 -> prob=0.4719 pred=0\n",
            "Weights: [(2, 1), (1,)]\n",
            "Logical gate \n",
            "OR - probabilities and rounding (threshold 0.5)\n",
            "0 OR 0 -> prob=0.5277 pred=1\n",
            "0 OR 1 -> prob=0.4386 pred=0\n",
            "1 OR 0 -> prob=0.6780 pred=1\n",
            "1 OR 1 -> prob=0.5956 pred=1\n",
            "Weights: [(2, 1), (1,)]\n",
            "Logical gate \n",
            "XOR - probabilities and rounding (threshold 0.5)\n",
            "0 XOR 0 -> prob=0.5050 pred=1\n",
            "0 XOR 1 -> prob=0.6483 pred=1\n",
            "1 XOR 0 -> prob=0.2854 pred=0\n",
            "1 XOR 1 -> prob=0.4191 pred=0\n",
            "Weights: [(2, 1), (1,)]\n"
          ]
        }
      ],
      "source": [
        "# DATA 준비\n",
        "X = np.array([\n",
        "    [0., 0.],\n",
        "    [0., 1.],\n",
        "    [1., 0.],\n",
        "    [1., 1.]\n",
        "])\n",
        "\n",
        "# Target labels for each task (AND, OR, XOR)\n",
        "y_AND = np.array([[0.], [0.], [0.], [1.]])\n",
        "y_OR  = np.array([[0.], [1.], [1.], [1.]])\n",
        "y_XOR = np.array([[0.], [1.], [1.], [0.]])\n",
        "\n",
        "# Defenite main functions(perceptron, mlp_xor, train)\n",
        "def perceptron(input_dim=2):        # input_dim - binary input (0 and 1) )\n",
        "    \"\"\"function perceptron (AND/OR).\"\"\"\n",
        "    model = keras.Sequential([\n",
        "                              # Sequential - model type (a linear stack of layers)\n",
        "        keras.layers.Dense(1,    # Dense - fully connected layer with 1 neuron\n",
        "                           activation='sigmoid',    # sigmoid - activation function (outputs values between 0 and 1)\n",
        "                           input_shape=(input_dim,)) # input_shape - shape of the input data (2 features)\n",
        "    ]) #\n",
        "    model.compile(  # compile the model with specified parameters\n",
        "        optimizer=keras.optimizers.SGD(learning_rate=0.1), # optimizer - Stochastic Gradient Descent with learning rate 0.1\n",
        "        loss='binary_crossentropy', # loss function - binary crossentropy (suitable for binary classification)\n",
        "        metrics=['accuracy'] # metrics - accuracy to evaluate the model's performance\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def mlp_xor(input_dim=2):\n",
        "    \"\"\"function mlp_xor (XOR)\"\"\"\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.Dense(4, # hidden layer with 4 neurons,because XOR is not linearly separable\n",
        "                           activation='sigmoid', #\n",
        "                           input_shape=(input_dim,)),\n",
        "        keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.SGD(learning_rate=0.1), # optimizer - Stochastic Gradient Descent with learning rate 0.1\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def train(name, model, X, y, epochs): # name : AND/OR/XOR , model - perceptron, X - bin data(0,1), y , epochs - number training\n",
        "    model.fit(X, y, epochs=epochs, verbose=0) # verbose=0 - no output during training\n",
        "    print(f\"Logical gate \\n{name} - probabilities and rounding (threshold 0.5)\") # if value > 0.5,then 1, else 0\n",
        "    for a, b in X:\n",
        "        p = model.predict(np.array([[a, b]], dtype=np.float32), verbose=0)[0, 0]\n",
        "        print(f\"{int(a)} {name} {int(b)} -> prob={p:.4f} pred={int(p>=0.5)}\")\n",
        "\n",
        "    print(\"Weights:\", [w.shape for w in model.get_weights()])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b193ee09",
      "metadata": {
        "id": "b193ee09"
      },
      "source": [
        "Training perceptron\n",
        "\n",
        "# Training models for AND, OR, XOR logical gates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3391d55f",
      "metadata": {
        "id": "3391d55f",
        "outputId": "29b8e8f6-ed3e-457f-dc54-950807dbf45b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logical gate \n",
            "AND - probabilities and rounding (threshold 0.5)\n",
            "0 AND 0 -> prob=0.4397 pred=0\n",
            "0 AND 1 -> prob=0.7470 pred=1\n",
            "1 AND 0 -> prob=0.7198 pred=1\n",
            "1 AND 1 -> prob=0.9063 pred=1\n",
            "Weights: [(2, 1), (1,)]\n",
            "Logical gate \n",
            "OR - probabilities and rounding (threshold 0.5)\n",
            "0 OR 0 -> prob=0.5492 pred=1\n",
            "0 OR 1 -> prob=0.4018 pred=0\n",
            "1 OR 0 -> prob=0.3941 pred=0\n",
            "1 OR 1 -> prob=0.2640 pred=0\n",
            "Weights: [(2, 1), (1,)]\n",
            "Logical gate \n",
            "XOR - probabilities and rounding (threshold 0.5)\n",
            "0 XOR 0 -> prob=0.4781 pred=0\n",
            "0 XOR 1 -> prob=0.6984 pred=1\n",
            "1 XOR 0 -> prob=0.6385 pred=1\n",
            "1 XOR 1 -> prob=0.8170 pred=1\n",
            "Weights: [(2, 1), (1,)]\n"
          ]
        }
      ],
      "source": [
        "# 1.Simulation situacion Underfitting (epochs=5) ~3 sec calculation\n",
        "\n",
        "# Training AND\n",
        "model_and = perceptron()\n",
        "train(\"AND\", model_and, X, y_AND, epochs=5) # epochs - число эпох обучения\n",
        "\n",
        "# Training OR\n",
        "model_or = perceptron()\n",
        "train(\"OR\", model_or, X, y_OR, epochs=5)\n",
        "\n",
        "# Training XOR(model - perceptron)\n",
        "model_xor = perceptron()\n",
        "train(\"XOR\", model_xor, X, y_XOR, epochs=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38d4d256",
      "metadata": {
        "id": "38d4d256",
        "outputId": "0d2e437f-a8c3-4dc8-847c-40ed091a7e23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logical gate \n",
            "AND - probabilities and rounding (threshold 0.5)\n",
            "0 AND 0 -> prob=0.0739 pred=0\n",
            "0 AND 1 -> prob=0.2803 pred=0\n",
            "1 AND 0 -> prob=0.2364 pred=0\n",
            "1 AND 1 -> prob=0.6017 pred=1\n",
            "Weights: [(2, 1), (1,)]\n",
            "Logical gate \n",
            "OR - probabilities and rounding (threshold 0.5)\n",
            "0 OR 0 -> prob=0.4603 pred=0\n",
            "0 OR 1 -> prob=0.8431 pred=1\n",
            "1 OR 0 -> prob=0.8525 pred=1\n",
            "1 OR 1 -> prob=0.9733 pred=1\n",
            "Weights: [(2, 1), (1,)]\n",
            "Logical gate \n",
            "XOR - probabilities and rounding (threshold 0.5)\n",
            "0 XOR 0 -> prob=0.5150 pred=1\n",
            "0 XOR 1 -> prob=0.5422 pred=1\n",
            "1 XOR 0 -> prob=0.4625 pred=0\n",
            "1 XOR 1 -> prob=0.4898 pred=0\n",
            "Weights: [(2, 1), (1,)]\n"
          ]
        }
      ],
      "source": [
        "# 1.Simulation situacion derfitting (epochs>300) ~40 sec calculation!\n",
        "\n",
        "# Training AND\n",
        "model_and = perceptron()\n",
        "train(\"AND\", model_and, X, y_AND, epochs=300) # epochs - number training\n",
        "# Training OR\n",
        "model_or = perceptron()\n",
        "train(\"OR\", model_or, X, y_OR, epochs=300)\n",
        "\n",
        "# Training XOR(model - perceptron)\n",
        "model_xor = perceptron()\n",
        "train(\"XOR\", model_xor, X, y_XOR, epochs=300)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b193fe84",
      "metadata": {
        "id": "b193fe84"
      },
      "source": [
        "# Problem XOR\n",
        "A simple perceptron model is not suitable for learning the XOR logical gate, because the XOR function is not linearly separable. Therefore, a more complex model (such as a multilayer perceptron with a hidden layer) will be used for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbd22013",
      "metadata": {
        "id": "dbd22013",
        "outputId": "9e5e692e-a92d-4f17-cfc9-005e82847b9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logical gate \n",
            "XOR - probabilities and rounding (threshold 0.5)\n",
            "0 XOR 0 -> prob=0.0113 pred=0\n",
            "0 XOR 1 -> prob=0.9893 pred=1\n",
            "1 XOR 0 -> prob=0.9895 pred=1\n",
            "1 XOR 1 -> prob=0.0114 pred=0\n",
            "Weights: [(2, 4), (4,), (4, 1), (1,)]\n"
          ]
        }
      ],
      "source": [
        "# Training XOR with MLP(Success only big epochs>10000) ~7minutes calculation!!!!!!!!!\n",
        "model_xor = mlp_xor()\n",
        "train(\"XOR\", model_xor, X, y_XOR, epochs=10000)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.13.1)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}